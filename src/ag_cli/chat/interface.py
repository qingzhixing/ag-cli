# chat/interface.py
from rich.panel import Panel
from rich.markdown import Markdown
from rich.live import Live
import re
import time


class ChatInterface:
    """èŠå¤©ç•Œé¢ç®¡ç†ç±»"""

    def __init__(self, client, console):
        self.client = client
        self.console = console
        self.system_prompt = "(å¦‚æœæœªæŒ‡å®šè¯­è¨€ï¼Œå›å¤ç­”æ¡ˆæ—¶è¯·ä½¿ç”¨ä¸­æ–‡è¯­è¨€)"

    def display_question(self, question):
        """æ˜¾ç¤ºé—®é¢˜"""
        self.console.print(
            Panel.fit(
                f"[bold cyan]{question}[/bold cyan]",
                title="[bold blue]QuestionğŸ’­[/bold blue]",
                border_style="blue",
            )
        )

    def display_streaming_response(self, response_stream):
        """åŠ¨æ€æ˜¾ç¤ºæµå¼AIå›å¤ - ä¼˜åŒ–Markdownç‰ˆæœ¬"""
        self.console.print("\n[bold green]ğŸ¤–:[/bold green]")

        full_response = ""
        last_update_time = time.time()
        update_interval = 0.3  # é™ä½åˆ·æ–°ç‡åˆ°300æ¯«ç§’
        chunk_buffer = ""  # ç¼“å†²åŒºï¼Œç´¯ç§¯ä¸€å®šå†…å®¹å†æ›´æ–°

        # ä¼˜åŒ–Liveé…ç½®ï¼šå¢åŠ ç¼“å†²åŒºå¤§å°ï¼Œé™ä½åˆ·æ–°ç‡
        with Live(refresh_per_second=5, auto_refresh=False) as live:
            try:
                for chunk in response_stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        chunk_buffer += content

                        # ç´¯ç§¯ä¸€å®šå†…å®¹æˆ–è¾¾åˆ°æ—¶é—´é—´éš”æ‰æ›´æ–°
                        current_time = time.time()
                        if (
                            current_time - last_update_time >= update_interval
                            or len(chunk_buffer) >= 100
                        ):  # ç´¯ç§¯100å­—ç¬¦æˆ–è¾¾åˆ°æ—¶é—´é—´éš”
                            # é¢„å¤„ç†å¹¶æ˜¾ç¤ºMarkdown
                            processed_response = self._preprocess_response(
                                full_response
                            )
                            markdown = Markdown(processed_response)
                            live.update(markdown, refresh=True)

                            # é‡ç½®è®¡æ—¶å™¨å’Œç¼“å†²åŒº
                            last_update_time = current_time
                            chunk_buffer = ""

                # æµå¼å“åº”ç»“æŸåï¼Œç¡®ä¿æ˜¾ç¤ºå®Œæ•´å†…å®¹
                processed_response = self._preprocess_response(full_response)
                markdown = Markdown(processed_response)
                live.update(markdown, refresh=True)

            except Exception as e:
                # å¦‚æœæµå¼å“åº”å‡ºç°é”™è¯¯ï¼Œæ˜¾ç¤ºå½“å‰å·²æ”¶é›†çš„å†…å®¹
                self.console.print(f"[yellow]âš ï¸ æµå¼å“åº”ä¸­æ–­: {str(e)}[/yellow]")
                if full_response:
                    processed_response = self._preprocess_response(full_response)
                    markdown = Markdown(processed_response)
                    live.update(markdown, refresh=True)

        return full_response

    def display_response(self, response):
        """æ˜¾ç¤ºAIå›å¤"""
        self.console.print("\n[bold green]ğŸ¤–:[/bold green]")

        if response is None:
            self.console.print("[yellow]âœ–ï¸æ¨¡å‹è¿”å›äº†ç©ºå“åº”ã€‚[/yellow]")
        else:
            # é¢„å¤„ç†å“åº”ï¼Œç¡®ä¿ä»£ç å—æ­£ç¡®æ¸²æŸ“
            processed_response = self._preprocess_response(response)
            markdown = Markdown(processed_response)
            self.console.print(markdown)

    def _preprocess_response(self, response):
        """é¢„å¤„ç†å“åº”ï¼Œç¡®ä¿ä»£ç å—æ­£ç¡®æ¸²æŸ“"""
        # ç¡®ä¿ä»£ç å—æœ‰æ­£ç¡®çš„è¯­è¨€æ ‡è¯†
        response = re.sub(r"```(\w*)", r"```\1\n", response)
        response = re.sub(r"```\n", r"\n```\n", response)
        return response

    def call_api_single(self, question, model=None):
        """å•æ¬¡APIè°ƒç”¨ - åŠ¨æ€æµå¼è¾“å‡º"""
        question_with_lang = question + self.system_prompt

        # è·å–æµå¼å“åº”ï¼ˆä¸æ˜¾ç¤ºæ€è€ƒä¸­ï¼‰
        actual_model = (
            self.client.resolve_model_name(model)
            if model
            else self.client.config["default_model"]
        )

        response = self.client.client.chat.completions.create(
            model=actual_model,
            messages=[{"role": "user", "content": question_with_lang}],
            stream=True,
        )

        # åŠ¨æ€æ˜¾ç¤ºæµå¼å“åº”
        return self.display_streaming_response(response)

    def call_api_continuous(self, messages, model=None):
        """è¿ç»­å¯¹è¯APIè°ƒç”¨ - åŠ¨æ€æµå¼è¾“å‡º"""
        # è·å–æµå¼å“åº”ï¼ˆä¸æ˜¾ç¤ºæ€è€ƒä¸­ï¼‰
        actual_model = (
            self.client.resolve_model_name(model)
            if model
            else self.client.config["default_model"]
        )

        response = self.client.client.chat.completions.create(
            model=actual_model,
            messages=messages,
            stream=True,
        )

        # åŠ¨æ€æ˜¾ç¤ºæµå¼å“åº”
        return self.display_streaming_response(response)
